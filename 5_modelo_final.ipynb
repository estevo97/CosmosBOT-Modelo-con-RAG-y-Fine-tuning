{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "apikey = os.getenv(\"clave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import json\n",
    "\n",
    "# Cargar el índice FAISS\n",
    "index = faiss.read_index(\"faiss_index.index\")\n",
    "\n",
    "# Cargar los documentos originales\n",
    "with open(\"embeddings/document_texts.json\", \"r\") as f:\n",
    "    documentos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = apikey\n",
    "def get_embedding(text):\n",
    "    # Este es el método correcto para la versión >= 1.0.0\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def search(query_embedding, k=5):\n",
    "    # Buscar los k documentos más cercanos al embedding de la consulta\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), k)\n",
    "    return I, D\n",
    "\n",
    "def retrieve_context(query_text, k=5):\n",
    "    # Obtener el embedding de la consulta\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    \n",
    "    # Buscar los documentos más relevantes\n",
    "    indices, distances = search(query_embedding, k)\n",
    "    \n",
    "    # Recuperar los textos correspondientes (si el índice está en rango)\n",
    "    relevant_texts = [documentos[i] for i in indices[0] if i < len(documentos)]\n",
    "    \n",
    "    return \" \".join(relevant_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo fine-tuning con RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=apikey)\n",
    "def generate_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"ft:gpt-3.5-turbo-0125:personal::AwCV0n9Y\",  # Ajusta con el nombre de tu modelo fine-tuned\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Eres una persona que está teniendo nua conversación distendida con un divulgador de ciencia\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.9,  # Ajusta la creatividad de la respuesta (0.0 - 1.0)\n",
    "        max_tokens=1150,   # Número máximo de tokens en la respuesta\n",
    "        top_p=1,          # Control de muestreo por núcleo (0.0 - 1.0)\n",
    "        frequency_penalty=0,  # Penaliza repeticiones\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(query):\n",
    "    # Recupera contexto con RAG\n",
    "    context = retrieve_context(query)\n",
    "    \n",
    "    # Crea el prompt combinando el contexto con la consulta\n",
    "    prompt = f\"Contexto relevante: {context}\\nPregunta del usuario: {query}\"\n",
    "    \n",
    "    # Genera la respuesta del modelo\n",
    "    response = generate_response(prompt)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Bueno, resulta que Colón no era físico ni astrónomo... Digamos que Colón estaba dándole vueltas a la brújula sin   \n",
       "saber muy bien qué pasaba. Para él, todo este rollo era un misterio bastante enmarañado. Veía cómo su brújula      \n",
       "apuntaba de un lado de la estrella polar al otro conforme avanzaba la noche, y no tenía ni idea de lo que estaba   \n",
       "sucediendo. Según su diario de a bordo, al comienzo de la noche, las agujas noroesteaban y a la mañana             \n",
       "nordesteaban. La brújula, en vez de señalar directamente al norte, se iba un poquito al oeste al principio de la   \n",
       "noche y luego un poquito al este justo antes del amanecer.inceton, en Nueva Jersey, es de unos 12 grados al oeste. \n",
       "Vas comprendiendo, ¿no?                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Bueno, resulta que Colón no era físico ni astrónomo... Digamos que Colón estaba dándole vueltas a la brújula sin   \n",
       "saber muy bien qué pasaba. Para él, todo este rollo era un misterio bastante enmarañado. Veía cómo su brújula      \n",
       "apuntaba de un lado de la estrella polar al otro conforme avanzaba la noche, y no tenía ni idea de lo que estaba   \n",
       "sucediendo. Según su diario de a bordo, al comienzo de la noche, las agujas noroesteaban y a la mañana             \n",
       "nordesteaban. La brújula, en vez de señalar directamente al norte, se iba un poquito al oeste al principio de la   \n",
       "noche y luego un poquito al este justo antes del amanecer.inceton, en Nueva Jersey, es de unos 12 grados al oeste. \n",
       "Vas comprendiendo, ¿no?                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "query = \"Qué descubrió Colón sobre la estrella POlar durante su expedición? Qué le marcaba su brújula?\"\n",
    "response = ask_model(query)\n",
    "console = Console()\n",
    "console.print(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mi_nuevo_entorno)",
   "language": "python",
   "name": "mi_nuevo_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
